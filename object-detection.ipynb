{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Object detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N6geSftN_E8"
      },
      "source": [
        "# !rm -rf data utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1cVrKadJyTM",
        "outputId": "7e9ca5e5-3f6d-44bb-b5f5-e3bd96245046",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile([*files.upload()][0], 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5d4f0f62-c171-4fc9-868b-163de772b311\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5d4f0f62-c171-4fc9-868b-163de772b311\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving SSD-Object-Detection-in-PyTorch-.zip to SSD-Object-Detection-in-PyTorch-.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byez9H5ZrZp4"
      },
      "source": [
        "# prepare_data\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "data_dir = \"./data\"\n",
        "weight_dir = \"./data/weights\"\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "if not os.path.exists(weight_dir):\n",
        "    os.mkdir(weight_dir)\n",
        "\n",
        "url = \"https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\"\n",
        "target_path = os.path.join(weight_dir, \"vgg16_reducedfc.pth\")\n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\")\n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "\n",
        "    tar = tarfile.TarFile(target_path)\n",
        "    tar.extractall(data_dir)\n",
        "    tar.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cczq7AUlKY6k",
        "outputId": "dab0df0c-9a28-4ea7-f7bc-e3474e36e983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip3 install torch==1.3.0 torchvision==0.4.1\n",
        "!pip3 install 'pillow<7.0.0'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.3.0 in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: torchvision==0.4.1 in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.0) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.1) (6.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.1) (1.15.0)\n",
            "Requirement already satisfied: pillow<7.0.0 in /usr/local/lib/python3.6/dist-packages (6.2.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK_Hy1PvyA0q"
      },
      "source": [
        "import os.path as osp\n",
        "\n",
        "import random\n",
        "import xml.etree.ElementTree as ET \n",
        "import cv2 \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import itertools\n",
        "from math import sqrt\n",
        "import time \n",
        "\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgpKld0TsZuI"
      },
      "source": [
        "# make_datapath\n",
        "\n",
        "def make_datapath_list(root_path):\n",
        "    image_path_template = osp.join(root_path, \"JPEGImages\", \"%s.jpg\")\n",
        "    annotation_path_template = osp.join(root_path, \"Annotations\", \"%s.xml\")\n",
        "\n",
        "    train_id_names = osp.join(root_path, \"ImageSets/Main/train.txt\")\n",
        "    val_id_names = osp.join(root_path, \"ImageSets/Main/val.txt\")\n",
        "\n",
        "    train_img_list = list()\n",
        "    train_annotation_list = list()\n",
        "\n",
        "    val_img_list = list()\n",
        "    val_annotation_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip() #xoá ký tự xuống dòng, xoá space\n",
        "        img_path = (image_path_template % file_id)\n",
        "        anno_path = (annotation_path_template % file_id)\n",
        "\n",
        "        train_img_list.append(img_path)\n",
        "        train_annotation_list.append(anno_path)\n",
        "    \n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()\n",
        "        img_path = (image_path_template % file_id)\n",
        "        anno_path = (annotation_path_template % file_id)\n",
        "\n",
        "        val_img_list.append(img_path)\n",
        "        val_annotation_list.append(anno_path)\n",
        "\n",
        "    return train_img_list, train_annotation_list, val_img_list, val_annotation_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOmrtV7zsZxQ"
      },
      "source": [
        "# extract information annotation\n",
        "\n",
        "class Annotation_xml(object):\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "    \n",
        "    def __call__(self, xml_path, width, height):\n",
        "        # include image annotation\n",
        "        ret = []\n",
        "        # read file xml\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "        \n",
        "        for obj in xml.iter('object'):\n",
        "            difficult = int(obj.find(\"difficult\").text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "            # information for bounding box    \n",
        "            bndbox = []\n",
        "            name = obj.find(\"name\").text.lower().strip()\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            pts = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
        "            for pt in pts:\n",
        "                pixel = int(bbox.find(pt).text) - 1\n",
        "                if pt == \"xmin\" or pt == \"xmax\":\n",
        "                    pixel /= width # ratio of width\n",
        "                else:\n",
        "                    pixel /= height # ratio of height \n",
        "                bndbox.append(pixel)\n",
        "            label_id = self.classes.index(name)\n",
        "            bndbox.append(label_id)\n",
        "            ret += [bndbox]\n",
        "        return np.array(ret) #[[xmin, ymin, xmax, ymax, label_id], ......]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRjrv1VWsZ6z"
      },
      "source": [
        "# transform\n",
        "\n",
        "from utils.augmentation import Compose, ConvertFromInts, ToAbsoluteCoords, \\\n",
        "    PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, \\\n",
        "    ToPercentCoords, Resize, SubtractMeans\n",
        "\n",
        "class DataTransform():\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            \"train\": Compose([\n",
        "                ConvertFromInts(), # convert image from int to float 32\n",
        "                ToAbsoluteCoords(), # back annotation to normal type\n",
        "                PhotometricDistort(), # change color by random\n",
        "                Expand(color_mean), \n",
        "                RandomSampleCrop(), # randomcrop image\n",
        "                RandomMirror(), # xoay ảnh ngược lại\n",
        "                ToPercentCoords(), # chuẩn hoá annotation data về dạng [0-1]\n",
        "                Resize(input_size),\n",
        "                SubtractMeans(color_mean) # Subtract mean của BGR\n",
        "            ]), \n",
        "            \"val\": Compose([\n",
        "                ConvertFromInts(), # convert image from int to float 32\n",
        "                Resize(input_size),\n",
        "                SubtractMeans(color_mean)\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "        return self.data_transform[phase](img, boxes, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmA1w2pssZ98"
      },
      "source": [
        "# create class datasets\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, img_list, anno_list, phase, transform, anno_xml):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "        self.anno_xml = anno_xml\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img, gt, height, width = self.pull_item(index)\n",
        "\n",
        "        return img, gt\n",
        "    \n",
        "    def pull_item(self, index):\n",
        "        img_file_path = self.img_list[index]\n",
        "        img = cv2.imread(img_file_path) #BGR\n",
        "        height, width, channels = img.shape\n",
        "\n",
        "        # get anno information\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        ann_info = self.anno_xml(anno_file_path, width, height)\n",
        "\n",
        "        # preprocessing\n",
        "        img, boxes, labels = self.transform(img, self.phase, ann_info[:, :4], ann_info[:, 4])\n",
        "\n",
        "        # BGR -> RGB, (height, width, channels) -> (channels, height, width)\n",
        "        img = torch.from_numpy(img[:,:,(2,1,0)]).permute(2,0,1)\n",
        "\n",
        "        # ground truth\n",
        "        gt = gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width\n",
        "\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    targets = []\n",
        "    imgs = []\n",
        "\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0]) #sample[0]=img\n",
        "        targets.append(torch.FloatTensor(sample[1])) # sample[1]=annotation\n",
        "    #[3, 300, 300]\n",
        "    # (batch_size, 3, 300, 300)\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    return imgs, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eMWZxiEsaEO"
      },
      "source": [
        "# create model and detect\n",
        "\n",
        "def create_vgg():\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "\n",
        "    cfgs = [64, 64, 'M', 128, 128, 'M',\n",
        "            256, 256, 256, 'MC', 512, 512, 512, 'M',\n",
        "            512, 512, 512]\n",
        "\n",
        "    for cfg in cfgs:\n",
        "        if cfg == 'M': #floor\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif cfg == 'MC': #ceiling\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, cfg, kernel_size=3, padding=1)\n",
        "\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = cfg\n",
        "        \n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "    layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "def create_extras():\n",
        "    layers = []\n",
        "    in_channels = 1024\n",
        "    cfgs = [256, 512, 128, 256, 128, 256, 128, 256]\n",
        "\n",
        "    layers += [nn.Conv2d(in_channels, cfgs[0], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[0], cfgs[1], kernel_size=3, stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfgs[1], cfgs[2], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[2], cfgs[3], kernel_size=3, stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfgs[3], cfgs[4], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[4], cfgs[5], kernel_size=3)]\n",
        "    layers += [nn.Conv2d(cfgs[5], cfgs[6], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[6], cfgs[7], kernel_size=3)]\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "def create_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
        "    loc_layers = []\n",
        "    conf_layers = []\n",
        "\n",
        "    # source1\n",
        "    # loc\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]*4, kernel_size=3, padding=1)]\n",
        "    # conf\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source2\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]*4, kernel_size=3, padding=1)]\n",
        "    #conf\n",
        "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source3\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source4\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source5\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source6\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    \"num_classes\": 21, #VOC data include 20 class + 1 background class\n",
        "    \"input_size\": 300, #SSD300\n",
        "    \"bbox_aspect_num\": [4, 6, 6, 6, 4, 4], # Tỷ lệ khung hình cho source1->source6`\n",
        "    \"feature_maps\": [38, 19, 10, 5, 3, 1],\n",
        "    \"steps\": [8, 16, 32, 64, 100, 300], # Size of default box\n",
        "    \"min_size\": [30, 60, 111, 162, 213, 264], # Size of default box\n",
        "    \"max_size\": [60, 111, 162, 213, 264, 315], # Size of default box\n",
        "    \"aspect_ratios\": [[2], [2,3], [2,3], [2,3], [2], [2]]\n",
        "}\n",
        "\n",
        "\n",
        "class SSD(nn.Module):\n",
        "    def __init__(self, phase, cfg):\n",
        "        super(SSD, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.num_classes= cfg[\"num_classes\"]\n",
        "\n",
        "        #create main modules\n",
        "        self.vgg = create_vgg()\n",
        "        self.extras = create_extras()\n",
        "        self.loc, self.conf = create_loc_conf(cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n",
        "        self.L2Norm = L2Norm()\n",
        "\n",
        "        #create default box\n",
        "        dbox = DefBox(cfg)\n",
        "        self.dbox_list = dbox.create_defbox()\n",
        "\n",
        "        if phase == \"inference\":\n",
        "            self.detect = Detect()\n",
        "\n",
        "    def forward(self, x):\n",
        "        sources = list()\n",
        "        loc = list()\n",
        "        conf = list()\n",
        "\n",
        "        for k in range(23):\n",
        "            x = self.vgg[k](x)\n",
        "        \n",
        "        # source1\n",
        "        source1 = self.L2Norm(x)\n",
        "        sources.append(source1)\n",
        "\n",
        "        for k in range(23, len(self.vgg)):\n",
        "            x = self.vgg[k](x)\n",
        "        # source2\n",
        "        sources.append(x)\n",
        "\n",
        "        # source3~6\n",
        "        for k, v in enumerate(self.extras):\n",
        "            x = F.relu(v(x), inplace=True)\n",
        "            if k %2 == 1:\n",
        "                sources.append(x)\n",
        "        \n",
        "\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
        "            # aspect_ratio_num = 4, 6\n",
        "            # (batch_size, 4*aspect_ratio_num, featuremap_height, featuremap_width)\n",
        "            # -> (batch_size, featuremap_height, featuremap_width ,4*aspect_ratio_num)\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1) #(batch_size, 34928) 4*8732\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1) #(batch_size, 8732*21)\n",
        "\n",
        "        loc = loc.view(loc.size(0), -1, 4) #(batch_size, 8732, 4)\n",
        "        conf = conf.view(conf.size(0), -1, self.num_classes) #(batch_size, 8732, 21)\n",
        "\n",
        "        output = (loc, conf, self.dbox_list)\n",
        "\n",
        "        if self.phase == \"inference\":\n",
        "            return self.detect(output[0], output[1], output[2])\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "def decode(loc, defbox_list):\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "    loc: [8732, 4] (delta_x, delta_y, delta_w, delta_h)\n",
        "    defbox_list: [8732, 4] (cx_d, cy_d, w_d, h_d)\n",
        "    returns:\n",
        "    boxes [xmin, ymin, xmax, ymax]\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = torch.cat((\n",
        "        defbox_list[:, :2] + 0.1*loc[:, :2]*defbox_list[:, 2:],\n",
        "        defbox_list[:, 2:]*torch.exp(loc[:,2:]*0.2)), dim=1)\n",
        "\n",
        "    boxes[:, :2] -= boxes[:,2:]/2 #calculate xmin, ymin\n",
        "    boxes[:, 2:] += boxes[:, :2] #calculate xmax, ymax\n",
        "\n",
        "    return boxes\n",
        "\n",
        "\n",
        "# non-maximum_supression\n",
        "def nms(boxes, scores, overlap=0.45, top_k=200):\n",
        "    \"\"\"\n",
        "    boxes: [num_box, 4]\n",
        "    scores: [num_box]\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    keep = scores.new(scores.size(0)).zero_().long()\n",
        "\n",
        "    # boxes coordinate\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    # area of boxes\n",
        "    area = torch.mul(x2-x1, y2-y1)\n",
        "\n",
        "    tmp_x1 = boxes.new()\n",
        "    tmp_x2 = boxes.new()\n",
        "    tmp_y1 = boxes.new()\n",
        "    tmp_y2 = boxes.new()\n",
        "    tmp_w = boxes.new()\n",
        "    tmp_h = boxes.new()\n",
        "\n",
        "    value, idx = scores.sort(0)\n",
        "    idx = idx[-top_k:] # id của top 200 boxes có độ tự tin cao nhất\n",
        "\n",
        "    while idx.numel() > 0:\n",
        "        i = idx[-1] # id của box có độ tự tin cao nhất\n",
        "        keep[count] = i\n",
        "        count += 1\n",
        "\n",
        "        if idx.size(0) == 1:\n",
        "            break\n",
        "        \n",
        "        idx = idx[:-1] #id của boxes ngoại trừ box có độ tự tin cao nhất\n",
        "        #information boxes\n",
        "        torch.index_select(x1, 0, idx, out=tmp_x1) #x1\n",
        "        torch.index_select(y1, 0, idx, out=tmp_y1) #y1\n",
        "        torch.index_select(x2, 0, idx, out=tmp_x2) #x2\n",
        "        torch.index_select(y2, 0, idx, out=tmp_y2) #y2\n",
        "\n",
        "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i]) # =x1[i] if tmp_x1 < x1[1]\n",
        "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n",
        "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n",
        "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i]) # =y2[i] if tmp_y2 > y2[i]\n",
        "        \n",
        "        # chuyển về tensor có size mà index được giảm đi 1\n",
        "        tmp_w.resize_as_(tmp_x2)\n",
        "        tmp_h.resize_as_(tmp_y2)\n",
        "\n",
        "        tmp_w = tmp_x2 - tmp_x1\n",
        "        tmp_h = tmp_y2 - tmp_y1\n",
        "\n",
        "        tmp_w = torch.clamp(tmp_w, min=0.0)\n",
        "        tmp_h = torch.clamp(tmp_h, min=0.0)\n",
        "\n",
        "        # overlap area\n",
        "        inter = tmp_w*tmp_h\n",
        "        others_area = torch.index_select(area, 0, idx) # diện tích của mỗi bbox\n",
        "        union = area[i] + others_area - inter\n",
        "        iou = inter/union\n",
        "        idx = idx[iou.le(overlap)] # giữ lại id của box có overlap ít với bbox đang xét\n",
        "\n",
        "    return keep, count\n",
        "\n",
        "\n",
        "class Detect(Function):\n",
        "    def __init__(self, conf_thresh=0.01, top_k=200, nsm_thresh=0.45):\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.conf_thresh = conf_thresh\n",
        "        self.top_k = top_k\n",
        "        self.nms_thresh = nsm_thresh\n",
        "\n",
        "    def forward(self, loc_data, conf_data, dbox_list):\n",
        "        num_batch = loc_data.size(0) #batch_size (2,4,6,...32, 64, 128)\n",
        "        num_dbox = loc_data.size(1) # 8732\n",
        "        num_classe = conf_data.size(2) #21\n",
        "\n",
        "        conf_data = self.softmax(conf_data) \n",
        "        # (batch_num, num_dbox, num_class) -> (batch_num, num_class, num_dbox)\n",
        "        conf_preds = conf_data.transpose(2, 1)\n",
        "\n",
        "        output = torch.zeros(num_batch, num_classe, self.top_k, 5)\n",
        "\n",
        "        # xử lý từng bức ảnh trong một batch các bức ảnh\n",
        "        for i in range(num_batch):\n",
        "            # Tính bbox từ offset information và default box\n",
        "            decode_boxes = decode(loc_data[i], dbox_list)\n",
        "\n",
        "            # copy confidence score của ảnh thứ i\n",
        "            conf_scores = conf_preds[i].clone()\n",
        "\n",
        "            for cl in range(1, num_classe):\n",
        "                c_mask = conf_scores[cl].gt(self.conf_thresh) # chỉ lấy những confidence > 0.01\n",
        "                scores = conf_scores[cl][c_mask]\n",
        "                if scores.nelement() == 0: #numel()\n",
        "                    continue\n",
        "\n",
        "                # đưa chiều về giống chiều của decode_boxes để tính toán\n",
        "                l_mask = c_mask.unsqueeze(1).expand_as(decode_boxes) #(8732, 4)\n",
        "                boxes = decode_boxes[l_mask].view(-1, 4) # (số box có độ tự tin lớn hơn > 0.01, 4)\n",
        "                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n",
        "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1), boxes[ids[:count]]), 1)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2BD8NNLFfHy"
      },
      "source": [
        "# l2\n",
        "\n",
        "class L2Norm(nn.Module):\n",
        "    def __init__(self, input_channels=512, scale=20):\n",
        "        super(L2Norm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_channels))\n",
        "        self.scale = scale\n",
        "        self.reset_parameters()\n",
        "        self.eps = 1e-10\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        nn.init.constant_(self.weight, self.scale)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x.size() = (batch_size, chanenel, height, width)\n",
        "        # L2Norm\n",
        "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps\n",
        "        x = torch.div(x, norm)\n",
        "        #weight.size() = (512) -> (1,512,1,1)\n",
        "        weights = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "\n",
        "        return weights*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vGL56JgFfK7"
      },
      "source": [
        "# default_box\n",
        "\n",
        "cfg = {\n",
        "    \"num_classes\": 21, #VOC data include 20 class + 1 background class\n",
        "    \"input_size\": 300, #SSD300\n",
        "    \"bbox_aspect_num\": [4, 6, 6, 6, 4, 4], # Tỷ lệ khung hình cho source1->source6`\n",
        "    \"feature_maps\": [38, 19, 10, 5, 3, 1],\n",
        "    \"steps\": [8, 16, 32, 64, 100, 300], # Size of default box\n",
        "    \"min_size\": [30, 60, 111, 162, 213, 264], # Size of default box\n",
        "    \"max_size\": [60, 111, 162, 213, 264, 315], # Size of default box\n",
        "    \"aspect_ratios\": [[2], [2,3], [2,3], [2,3], [2], [2]]\n",
        "}\n",
        "\n",
        "\n",
        "class DefBox():\n",
        "    def __init__(self, cfg):\n",
        "        self.img_size = cfg[\"input_size\"]\n",
        "        self.feature_maps = cfg[\"feature_maps\"]\n",
        "        self.min_size = cfg[\"min_size\"]\n",
        "        self.max_size = cfg[\"max_size\"]\n",
        "        self.aspect_ratios = cfg[\"aspect_ratios\"]\n",
        "        self.steps = cfg[\"steps\"]\n",
        "\n",
        "    def create_defbox(self):\n",
        "        defbox_list = []\n",
        "\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            for i, j in itertools.product(range(f), repeat=2):\n",
        "                f_k = self.img_size / self.steps[k]\n",
        "                cx = (j+0.5)/f_k\n",
        "                cy = (i+0.5)/f_k\n",
        "\n",
        "                # small square box \n",
        "                s_k = self.min_size[k]/self.img_size #first case: 30/300\n",
        "                defbox_list += [cx, cy, s_k, s_k]\n",
        "\n",
        "                # big square box\n",
        "                s_k_ = sqrt(s_k*(self.max_size[k]/self.img_size))\n",
        "                defbox_list += [cx, cy, s_k_, s_k_]\n",
        "\n",
        "                for ar in self.aspect_ratios[k]:\n",
        "                    defbox_list += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
        "                    defbox_list += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
        "                \n",
        "        output = torch.Tensor(defbox_list).view(-1, 4)\n",
        "        output.clamp_(max = 1, min=0)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw0yIR7bFfN8"
      },
      "source": [
        "# multiboxloss\n",
        "\n",
        "# Jaccard: \n",
        "# Hard negative mining: negative default box = 3times positve default bos\n",
        "# Loss in regression task: MSE ->F.SmoothL1Loss\n",
        "# Loss in classification (multi class): F.CrossEntropy\n",
        "\n",
        "from utils.box_utils import match\n",
        "\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    def __init__(self, jaccard_threshold=0.5, neg_pos=3, device=\"cpu\"):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.jaccard_threshold = jaccard_threshold\n",
        "        self.neg_pos = neg_pos\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, predictions, targets):\n",
        "        loc_data, conf_data, dbox_list = predictions\n",
        "\n",
        "        #(batch_num, num_dbox, num_classes)\n",
        "        num_batch = loc_data.size(0) \n",
        "        num_dbox = loc_data.size(1) #8732\n",
        "        num_classes = conf_data.size(2)\n",
        "\n",
        "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
        "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
        "\n",
        "        for idx in range(num_batch):\n",
        "            truths = targets[idx][:, :-1].to(self.device) #(xmin, ymin, xmax, ymax) BBox\n",
        "            labels = targets[idx][:, -1].to(self.device) #label\n",
        "\n",
        "            dbox = dbox_list.to(self.device)\n",
        "            variances = [0.1, 0.2]\n",
        "            match(self.jaccard_threshold, truths, dbox, variances, labels, loc_t, conf_t_label, idx)\n",
        "\n",
        "        #SmoothL1Loss\n",
        "        pos_mask = conf_t_label > 0\n",
        "        # loc_data(num_batch, 8732, 4)\n",
        "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
        "\n",
        "        # positive dbox, loc_data\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "        loss_loc = F.smooth_l1_loss(loc_p, loc_t, reduction=\"sum\")\n",
        "\n",
        "        #loss_conf\n",
        "        #CrossEntropy\n",
        "        batch_conf = conf_data.view(-1, num_classes) #(num_batch*num_box, num_classes)\n",
        "        loss_conf = F.cross_entropy(batch_conf, conf_t_label.view(-1), reduction=\"none\")\n",
        "\n",
        "        # hard negative mining\n",
        "        num_pos = pos_mask.long().sum(1, keepdim=True)\n",
        "        loss_conf = loss_conf.view(num_batch, -1) # torch.size([num_batch, 8732])\n",
        "\n",
        "        _, loss_idx = loss_conf.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "        # idx_rank chính là thông số để biết được độ lớn loss nằm ở vị trí bao nhiêu\n",
        "\n",
        "        num_neg = torch.clamp(num_pos*self.neg_pos, max=num_dbox)\n",
        "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
        "\n",
        "        #(num_batch, 8732) -> (num_batch, 8732, 21)\n",
        "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
        "        conf_t_pre = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)].view(-1, num_classes)\n",
        "        conf_t_label_ = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
        "        loss_conf = F.cross_entropy(conf_t_pre, conf_t_label_, reduction=\"sum\")\n",
        "\n",
        "        # total loss = loss_loc + loss_conf\n",
        "        N = num_pos.sum()\n",
        "        loss_loc = loss_loc/N\n",
        "        loss_conf = loss_conf/N\n",
        "\n",
        "        return loss_loc, loss_conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khMvKak-GQSM"
      },
      "source": [
        "# train\n",
        "\n",
        "# dataloader\n",
        "# network -> SSD300\n",
        "# loss -> MultiBoxLoss\n",
        "# optimizer\n",
        "# training, validation\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# dataloader\n",
        "root_path = \"./data/VOCdevkit/VOC2012\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(root_path)\n",
        "\n",
        "classes = [\"aeroplane\", \"bicycle\", \"bird\",  \"boat\", \"bottle\", \n",
        "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "    \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
        "    \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "color_mean = (104, 117, 123)\n",
        "input_size = 300\n",
        "\n",
        "#img_list, anno_list, phase, transform, anno_xml\n",
        "train_dataset = Dataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(input_size, color_mean), anno_xml=Annotation_xml(classes))\n",
        "val_dataset = Dataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(input_size, color_mean), anno_xml=Annotation_xml(classes))\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
        "dataloader_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# network\n",
        "cfg = {\n",
        "    \"num_classes\": 21, #VOC data include 20 class + 1 background class\n",
        "    \"input_size\": 300, #SSD300\n",
        "    \"bbox_aspect_num\": [4, 6, 6, 6, 4, 4], # Tỷ lệ khung hình cho source1->source6`\n",
        "    \"feature_maps\": [38, 19, 10, 5, 3, 1],\n",
        "    \"steps\": [8, 16, 32, 64, 100, 300], # Size of default box\n",
        "    \"min_size\": [30, 60, 111, 162, 213, 264], # Size of default box\n",
        "    \"max_size\": [60, 111, 162, 213, 264, 315], # Size of default box\n",
        "    \"aspect_ratios\": [[2], [2,3], [2,3], [2,3], [2], [2]]\n",
        "}\n",
        "\n",
        "net = SSD(phase=\"train\", cfg=cfg)\n",
        "vgg_weights = torch.load(\"./data/weights/vgg16_reducedfc.pth\")\n",
        "net.vgg.load_state_dict(vgg_weights)\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "# He init\n",
        "net.extras.apply(weights_init)\n",
        "net.loc.apply(weights_init)\n",
        "net.conf.apply(weights_init)\n",
        "\n",
        "# MultiBoxLoss\n",
        "criterion = MultiBoxLoss(jaccard_threshold=0.5, neg_pos=3, device=device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# training, validation\n",
        "def train_model(net, dataloader_dict, criterion, optimizer, num_epochs):\n",
        "    # move network to GPU\n",
        "    net.to(device)\n",
        "\n",
        "    iteration = 1\n",
        "    epoch_train_loss = 0.0\n",
        "    epoch_val_loss = 0.0\n",
        "    logs = []\n",
        "    for epoch in range(num_epochs+1):\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        print(\"---\"*20)\n",
        "        print(\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
        "        print(\"---\"*20)\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                net.train()\n",
        "                print(\"(Training)\")\n",
        "            else:\n",
        "                if (epoch+1) % 10 == 0:\n",
        "                    net.eval() \n",
        "                    print(\"---\"*10)\n",
        "                    print(\"(Validation)\")\n",
        "                else:\n",
        "                    continue\n",
        "            for images, targets in dataloader_dict[phase]:\n",
        "                # move to GPU\n",
        "                images = images.to(device)\n",
        "                targets = [ann.to(device) for ann in targets]\n",
        "                # init optimizer\n",
        "                optimizer.zero_grad()\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase==\"train\"):\n",
        "                    outputs = net(images)\n",
        "                    loss_l, loss_c = criterion(outputs, targets)\n",
        "                    loss = loss_l + loss_c\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward() # calculate gradient\n",
        "                        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
        "                        optimizer.step() # update parameters\n",
        "\n",
        "                        if (iteration % 10) == 0:\n",
        "                            t_iter_end = time.time()\n",
        "                            duration = t_iter_end - t_iter_start\n",
        "                            print(\"Iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec\".format(iteration, loss.item(), duration))\n",
        "                            t_iter_start = time.time()\n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item()\n",
        "        t_epoch_end = time.time()\n",
        "        print(\"---\"*20)\n",
        "        print(\"Epoch {} || epoch_train_loss: {:.4f} || Epoch_val_loss: {:.4f}\".format(epoch+1, epoch_train_loss, epoch_val_loss))           \n",
        "        print(\"Duration: {:.4f} sec\".format(t_epoch_end - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        log_epoch = {\"epoch\": epoch+1, \"train_loss\": epoch_train_loss, \"val_loss\": epoch_val_loss}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"./data/ssd_logs.csv\")\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_val_loss = 0.0\n",
        "        if ((epoch+1) % 10 == 0):\n",
        "            torch.save(net.state_dict(), \"./data/weights/ssd300_\" + str(epoch+1) + \".pth\")\n",
        "\n",
        "num_epochs = 100\n",
        "train_model(net, dataloader_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beRbTISxV_cs"
      },
      "source": [
        "url = \"https://images.unsplash.com/flagged/photo-1563831175532-76e760e1d291?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1000&q=80\"\n",
        "target_path = os.path.join(data_dir, \"cowboy.jpg\")\n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "img = Image.open(target_path)\n",
        "new_image = img.resize((300, 300))\n",
        "new_image.save(target_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7WTZh75GQY4"
      },
      "source": [
        "# detect\n",
        "\n",
        "\n",
        "classes = [\"aeroplane\", \"bicycle\", \"bird\",  \"boat\", \"bottle\", \n",
        "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "    \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
        "    \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "cfg = {\n",
        "    \"num_classes\": 21, #VOC data include 20 class + 1 background class\n",
        "    \"input_size\": 300, #SSD300\n",
        "    \"bbox_aspect_num\": [4, 6, 6, 6, 4, 4], # Tỷ lệ khung hình cho source1->source6`\n",
        "    \"feature_maps\": [38, 19, 10, 5, 3, 1],\n",
        "    \"steps\": [8, 16, 32, 64, 100, 300], # Size of default box\n",
        "    \"min_size\": [30, 60, 111, 162, 213, 264], # Size of default box\n",
        "    \"max_size\": [60, 111, 162, 213, 264, 315], # Size of default box\n",
        "    \"aspect_ratios\": [[2], [2,3], [2,3], [2,3], [2], [2]]\n",
        "}\n",
        "\n",
        "net = SSD(phase=\"inference\", cfg=cfg)\n",
        "net_weights = torch.load(\"./data/weights/ssd300_100.pth\", map_location={\"cuda:0\":\"cpu\"})\n",
        "net.load_state_dict(net_weights)\n",
        "\n",
        "def show_predict(img_file_path):\n",
        "    img = cv2.imread(img_file_path)\n",
        "\n",
        "    color_mean = (104, 117, 123)\n",
        "    input_size = 300\n",
        "    transform = DataTransform(input_size, color_mean)\n",
        "\n",
        "    phase = \"val\"\n",
        "    img_tranformed, boxes, labels = transform(img, phase, \"\", \"\")\n",
        "    img_tensor = torch.from_numpy(img_tranformed[:,:,(2,1,0)]).permute(2,0,1)\n",
        "\n",
        "    net.eval()\n",
        "    input = img_tensor.unsqueeze(0) #(1, 3, 300, 300)\n",
        "    output = net(input)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    colors = [(255,0,0), (0,255,0), (0,0,255)]\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "    detections = output.data #(1, 21, 200, 5) 5: score, cx, cy, w, h\n",
        "    scale = torch.Tensor(img.shape[1::-1]).repeat(2)\n",
        "\n",
        "    for i in range(detections.size(1)):\n",
        "        j = 0\n",
        "        while detections[0, i, j, 0] >= 0.6:\n",
        "            score = detections[0, i, j, 0]\n",
        "            pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n",
        "            cv2.rectangle(img,\n",
        "                          (int(pt[0]), int(pt[1])),\n",
        "                          (int(pt[2]), int(pt[3])),\n",
        "                          colors[i%3], 2\n",
        "                          )\n",
        "            display_text = \"%s: %.2f\"%(classes[i-1], score)\n",
        "            cv2.putText(img, display_text, (int(pt[0]), int(pt[1])),\n",
        "                font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "            j += 1\n",
        "    \n",
        "    cv2.imshow(\"Result\", img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "img_file_path = target_path\n",
        "show_predict(img_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_oMZN2DTt0_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}