{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Object detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "byez9H5ZrZp4"
      },
      "source": [
        "# prepare_data\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "data_dir = \"./data\"\n",
        "weight_dir = \"./data/weights\"\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "if not os.path.exists(weight_dir):\n",
        "    os.mkdir(weight_dir)\n",
        "\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(weight_dir, \"vgg16_reducedfc.pth\")\n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve('https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth', target_path)\n",
        "\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\")\n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "\n",
        "    tar = tarfile.TarFile(target_path)\n",
        "    tar.extractall(data_dir)\n",
        "    tar.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK_Hy1PvyA0q"
      },
      "source": [
        "import os.path as osp\n",
        "\n",
        "import random\n",
        "import xml.etree.ElementTree as ET \n",
        "import cv2 \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import itertools\n",
        "from math import sqrt\n",
        "import time \n",
        "\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgpKld0TsZuI"
      },
      "source": [
        "# make_datapath\n",
        "\n",
        "def make_datapath_list(root_path):\n",
        "    image_path_template = osp.join(root_path, \"JPEGImages\", \"%s.jpg\")\n",
        "    annotation_path_template = osp.join(root_path, \"Annotations\", \"%s.xml\")\n",
        "\n",
        "    train_id_names = osp.join(root_path, \"ImageSets/Main/train.txt\")\n",
        "    val_id_names = osp.join(root_path, \"ImageSets/Main/val.txt\")\n",
        "\n",
        "    train_img_list = list()\n",
        "    train_annotation_list = list()\n",
        "\n",
        "    val_img_list = list()\n",
        "    val_annotation_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip() #xoá ký tự xuống dòng, xoá space\n",
        "        img_path = (image_path_template % file_id)\n",
        "        anno_path = (annotation_path_template % file_id)\n",
        "\n",
        "        train_img_list.append(img_path)\n",
        "        train_annotation_list.append(anno_path)\n",
        "    \n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()\n",
        "        img_path = (image_path_template % file_id)\n",
        "        anno_path = (annotation_path_template % file_id)\n",
        "\n",
        "        val_img_list.append(img_path)\n",
        "        val_annotation_list.append(anno_path)\n",
        "\n",
        "    return train_img_list, train_annotation_list, val_img_list, val_annotation_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOmrtV7zsZxQ"
      },
      "source": [
        "# extract information annotation\n",
        "\n",
        "class Annotation_xml(object):\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "    \n",
        "    def __call__(self, xml_path, width, height):\n",
        "        # include image annotation\n",
        "        ret = []\n",
        "        # read file xml\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "        \n",
        "        for obj in xml.iter('object'):\n",
        "            difficult = int(obj.find(\"difficult\").text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "            # information for bounding box    \n",
        "            bndbox = []\n",
        "            name = obj.find(\"name\").text.lower().strip()\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            pts = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
        "            for pt in pts:\n",
        "                pixel = int(bbox.find(pt).text) - 1\n",
        "                if pt == \"xmin\" or pt == \"xmax\":\n",
        "                    pixel /= width # ratio of width\n",
        "                else:\n",
        "                    pixel /= height # ratio of height \n",
        "                bndbox.append(pixel)\n",
        "            label_id = self.classes.index(name)\n",
        "            bndbox.append(label_id)\n",
        "            ret += [bndbox]\n",
        "        return np.array(ret) #[[xmin, ymin, xmax, ymax, label_id], ......]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRjrv1VWsZ6z"
      },
      "source": [
        "# transform\n",
        "\n",
        "from utils.augmentation import Compose, ConvertFromInts, ToAbsoluteCoords, \\\n",
        "    PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, \\\n",
        "    ToPercentCoords, Resize, SubtractMeans\n",
        "\n",
        "class DataTransform():\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            \"train\": Compose([\n",
        "                ConvertFromInts(), # convert image from int to float 32\n",
        "                ToAbsoluteCoords(), # back annotation to normal type\n",
        "                PhotometricDistort(), # change color by random\n",
        "                Expand(color_mean), \n",
        "                RandomSampleCrop(), # randomcrop image\n",
        "                RandomMirror(), # xoay ảnh ngược lại\n",
        "                ToPercentCoords(), # chuẩn hoá annotation data về dạng [0-1]\n",
        "                Resize(input_size),\n",
        "                SubtractMeans(color_mean) # Subtract mean của BGR\n",
        "            ]), \n",
        "            \"val\": Compose([\n",
        "                ConvertFromInts(), # convert image from int to float 32\n",
        "                Resize(input_size),\n",
        "                SubtractMeans(color_mean)\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "        return self.data_transform[phase](img, boxes, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmA1w2pssZ98"
      },
      "source": [
        "# create class datasets\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, img_list, anno_list, phase, transform, anno_xml):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "        self.anno_xml = anno_xml\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img, gt, height, width = self.pull_item(index)\n",
        "\n",
        "        return img, gt\n",
        "    \n",
        "    def pull_item(self, index):\n",
        "        img_file_path = self.img_list[index]\n",
        "        img = cv2.imread(img_file_path) #BGR\n",
        "        height, width, channels = img.shape\n",
        "\n",
        "        # get anno information\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        ann_info = self.anno_xml(anno_file_path, width, height)\n",
        "\n",
        "        # preprocessing\n",
        "        img, boxes, labels = self.transform(img, self.phase, ann_info[:, :4], ann_info[:, 4])\n",
        "\n",
        "        # BGR -> RGB, (height, width, channels) -> (channels, height, width)\n",
        "        img = torch.from_numpy(img[:,:,(2,1,0)]).permute(2,0,1)\n",
        "\n",
        "        # ground truth\n",
        "        gt = gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width\n",
        "\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    targets = []\n",
        "    imgs = []\n",
        "\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0]) #sample[0]=img\n",
        "        targets.append(torch.FloatTensor(sample[1])) # sample[1]=annotation\n",
        "    #[3, 300, 300]\n",
        "    # (batch_size, 3, 300, 300)\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    return imgs, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eMWZxiEsaEO"
      },
      "source": [
        "# create model and detect\n",
        "\n",
        "def create_vgg():\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "\n",
        "    cfgs = [64, 64, 'M', 128, 128, 'M',\n",
        "            256, 256, 256, 'MC', 512, 512, 512, 'M',\n",
        "            512, 512, 512]\n",
        "\n",
        "    for cfg in cfgs:\n",
        "        if cfg == 'M': #floor\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif cfg == 'MC': #ceiling\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, cfg, kernel_size=3, padding=1)\n",
        "\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = cfg\n",
        "        \n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "    layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "def create_extras():\n",
        "    layers = []\n",
        "    in_channels = 1024\n",
        "    cfgs = [256, 512, 128, 256, 128, 256, 128, 256]\n",
        "\n",
        "    layers += [nn.Conv2d(in_channels, cfgs[0], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[0], cfgs[1], kernel_size=3, stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfgs[1], cfgs[2], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[2], cfgs[3], kernel_size=3, stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfgs[3], cfgs[4], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[4], cfgs[5], kernel_size=3)]\n",
        "    layers += [nn.Conv2d(cfgs[5], cfgs[6], kernel_size=1)]\n",
        "    layers += [nn.Conv2d(cfgs[6], cfgs[7], kernel_size=3)]\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "def create_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
        "    loc_layers = []\n",
        "    conf_layers = []\n",
        "\n",
        "    # source1\n",
        "    # loc\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]*4, kernel_size=3, padding=1)]\n",
        "    # conf\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source2\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]*4, kernel_size=3, padding=1)]\n",
        "    #conf\n",
        "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source3\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source4\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source5\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    #source6\n",
        "    #loc\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]*4, kernel_size=3, padding=1)]\n",
        "    #conf \n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]*num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    \"num_classes\": 21, #VOC data include 20 class + 1 background class\n",
        "    \"input_size\": 300, #SSD300\n",
        "    \"bbox_aspect_num\": [4, 6, 6, 6, 4, 4], # Tỷ lệ khung hình cho source1->source6`\n",
        "    \"feature_maps\": [38, 19, 10, 5, 3, 1],\n",
        "    \"steps\": [8, 16, 32, 64, 100, 300], # Size of default box\n",
        "    \"min_size\": [30, 60, 111, 162, 213, 264], # Size of default box\n",
        "    \"max_size\": [60, 111, 162, 213, 264, 315], # Size of default box\n",
        "    \"aspect_ratios\": [[2], [2,3], [2,3], [2,3], [2], [2]]\n",
        "}\n",
        "\n",
        "\n",
        "class SSD(nn.Module):\n",
        "    def __init__(self, phase, cfg):\n",
        "        super(SSD, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.num_classes= cfg[\"num_classes\"]\n",
        "\n",
        "        #create main modules\n",
        "        self.vgg = create_vgg()\n",
        "        self.extras = create_extras()\n",
        "        self.loc, self.conf = create_loc_conf(cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n",
        "        self.L2Norm = L2Norm()\n",
        "\n",
        "        #create default box\n",
        "        dbox = DefBox(cfg)\n",
        "        self.dbox_list = dbox.create_defbox()\n",
        "\n",
        "        if phase == \"inference\":\n",
        "            self.detect = Detect()\n",
        "\n",
        "    def forward(self, x):\n",
        "        sources = list()\n",
        "        loc = list()\n",
        "        conf = list()\n",
        "\n",
        "        for k in range(23):\n",
        "            x = self.vgg[k](x)\n",
        "        \n",
        "        # source1\n",
        "        source1 = self.L2Norm(x)\n",
        "        sources.append(source1)\n",
        "\n",
        "        for k in range(23, len(self.vgg)):\n",
        "            x = self.vgg[k](x)\n",
        "        # source2\n",
        "        sources.append(x)\n",
        "\n",
        "        # source3~6\n",
        "        for k, v in enumerate(self.extras):\n",
        "            x = F.relu(v(x), inplace=True)\n",
        "            if k %2 == 1:\n",
        "                sources.append(x)\n",
        "        \n",
        "\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
        "            # aspect_ratio_num = 4, 6\n",
        "            # (batch_size, 4*aspect_ratio_num, featuremap_height, featuremap_width)\n",
        "            # -> (batch_size, featuremap_height, featuremap_width ,4*aspect_ratio_num)\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1) #(batch_size, 34928) 4*8732\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1) #(batch_size, 8732*21)\n",
        "\n",
        "        loc = loc.view(loc.size(0), -1, 4) #(batch_size, 8732, 4)\n",
        "        conf = conf.view(conf.size(0), -1, self.num_classes) #(batch_size, 8732, 21)\n",
        "\n",
        "        output = (loc, conf, self.dbox_list)\n",
        "\n",
        "        if self.phase == \"inference\":\n",
        "            return self.detect(output[0], output[1], output[2])\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "def decode(loc, defbox_list):\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "    loc: [8732, 4] (delta_x, delta_y, delta_w, delta_h)\n",
        "    defbox_list: [8732, 4] (cx_d, cy_d, w_d, h_d)\n",
        "    returns:\n",
        "    boxes [xmin, ymin, xmax, ymax]\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = torch.cat((\n",
        "        defbox_list[:, :2] + 0.1*loc[:, :2]*defbox_list[:, 2:],\n",
        "        defbox_list[:, 2:]*torch.exp(loc[:,2:]*0.2)), dim=1)\n",
        "\n",
        "    boxes[:, :2] -= boxes[:,2:]/2 #calculate xmin, ymin\n",
        "    boxes[:, 2:] += boxes[:, :2] #calculate xmax, ymax\n",
        "\n",
        "    return boxes\n",
        "\n",
        "\n",
        "# non-maximum_supression\n",
        "def nms(boxes, scores, overlap=0.45, top_k=200):\n",
        "    \"\"\"\n",
        "    boxes: [num_box, 4]\n",
        "    scores: [num_box]\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    keep = scores.new(scores.size(0)).zero_().long()\n",
        "\n",
        "    # boxes coordinate\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    # area of boxes\n",
        "    area = torch.mul(x2-x1, y2-y1)\n",
        "\n",
        "    tmp_x1 = boxes.new()\n",
        "    tmp_x2 = boxes.new()\n",
        "    tmp_y1 = boxes.new()\n",
        "    tmp_y2 = boxes.new()\n",
        "    tmp_w = boxes.new()\n",
        "    tmp_h = boxes.new()\n",
        "\n",
        "    value, idx = scores.sort(0)\n",
        "    idx = idx[-top_k:] # id của top 200 boxes có độ tự tin cao nhất\n",
        "\n",
        "    while idx.numel() > 0:\n",
        "        i = idx[-1] # id của box có độ tự tin cao nhất\n",
        "        keep[count] = i\n",
        "        count += 1\n",
        "\n",
        "        if idx.size(0) == 1:\n",
        "            break\n",
        "        \n",
        "        idx = idx[:-1] #id của boxes ngoại trừ box có độ tự tin cao nhất\n",
        "        #information boxes\n",
        "        torch.index_select(x1, 0, idx, out=tmp_x1) #x1\n",
        "        torch.index_select(y1, 0, idx, out=tmp_y1) #y1\n",
        "        torch.index_select(x2, 0, idx, out=tmp_x2) #x2\n",
        "        torch.index_select(y2, 0, idx, out=tmp_y2) #y2\n",
        "\n",
        "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i]) # =x1[i] if tmp_x1 < x1[1]\n",
        "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n",
        "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n",
        "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i]) # =y2[i] if tmp_y2 > y2[i]\n",
        "        \n",
        "        # chuyển về tensor có size mà index được giảm đi 1\n",
        "        tmp_w.resize_as_(tmp_x2)\n",
        "        tmp_h.resize_as_(tmp_y2)\n",
        "\n",
        "        tmp_w = tmp_x2 - tmp_x1\n",
        "        tmp_h = tmp_y2 - tmp_y1\n",
        "\n",
        "        tmp_w = torch.clamp(tmp_w, min=0.0)\n",
        "        tmp_h = torch.clamp(tmp_h, min=0.0)\n",
        "\n",
        "        # overlap area\n",
        "        inter = tmp_w*tmp_h\n",
        "        others_area = torch.index_select(area, 0, idx) # diện tích của mỗi bbox\n",
        "        union = area[i] + others_area - inter\n",
        "        iou = inter/union\n",
        "        idx = idx[iou.le(overlap)] # giữ lại id của box có overlap ít với bbox đang xét\n",
        "\n",
        "    return keep, count"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}